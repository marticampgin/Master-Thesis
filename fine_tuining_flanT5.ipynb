{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kMX85YasCWcE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30e5c7b3-e930-40d6-a79d-1498d0784b98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q bitsandbytes accelerate datasets evaluate rouge_score\n",
        "!pip install -q git+https://github.com/huggingface/peft.git@main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "47zM2nKJChBK"
      },
      "outputs": [],
      "source": [
        "from transformers import (AutoTokenizer,\n",
        "                          AutoModelForSeq2SeqLM,\n",
        "                          Seq2SeqTrainingArguments,\n",
        "                          Seq2SeqTrainer,\n",
        "                          DataCollatorForSeq2Seq)\n",
        "\n",
        "from peft import prepare_model_for_int8_training, LoraConfig, get_peft_model, TaskType\n",
        "from datasets import Dataset\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "import nltk\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import evaluate\n",
        "import random\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "from few_shot_testing import load_data\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/flan-t5-large\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name, load_in_8bit=True)\n",
        "model = prepare_model_for_int8_training(model)"
      ],
      "metadata": {
        "id": "dkroB9N8xf6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "LHNo30hDUVsf"
      },
      "outputs": [],
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJEg9lk6UakG",
        "outputId": "be12815d-0954-490b-c0b0-264570700cbe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 30670848 || all params: 813820928 || trainable%: 3.768746531915188\n"
          ]
        }
      ],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32, lora_alpha=64,\n",
        "    target_modules=[\"q\", \"v\", \"k\", \"o\", \"wi_0\", \"wi_1\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_2_SEQ_LM\"\n",
        ")\n",
        "\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "few_shot_samples = 51\n",
        "max_len = 512\n",
        "model_name = None\n",
        "\n",
        "seed = 77\n",
        "num_of_runs = 3\n",
        "stratify_seeds = [77, 88, 99]\n",
        "\n",
        "train_ds, test_ds = load_data(\"train_data.csv\",\n",
        "                              \"test_data.csv\",\n",
        "                              few_shot_samples,\n",
        "                              tokenizer,\n",
        "                              512,\n",
        "                              model_name,\n",
        "                              stratify_seeds[0],\n",
        "                              text2text=True)\n",
        "\n",
        "int2str = {-1: 'negative', 0: 'neutral', 1: 'positive'}\n",
        "str2int = {v:k for k, v in int2str.items()}"
      ],
      "metadata": {
        "id": "_E7emJ2AAgKs"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sample(text):\n",
        "  return f\"\"\"Perform Sentiment classification task.\n",
        "Given the text assign a sentiment label from ['negative', 'positive', 'neutral'].\n",
        "Return label only without any other text.\n",
        "\n",
        "<text>: {text}\n",
        "<sentiment>: \"\"\".strip()\n",
        "\n",
        "def preprocess_function(examples):\n",
        "  inputs = [generate_sample(text) for text in examples['text']]\n",
        "  model_inputs = tokenizer(inputs, max_length=512,  truncation=True)\n",
        "\n",
        "  # The labels are tokenized outputs\n",
        "  labels = tokenizer(text_target=examples['label'],\n",
        "                     max_length=512,\n",
        "                     truncation=True)\n",
        "\n",
        "  model_inputs['labels'] = labels['input_ids']\n",
        "\n",
        "  return model_inputs\n",
        "\n",
        "\n",
        "tokenized_train_dataset = train_ds.map(preprocess_function, batched=True)\n",
        "tokenized_test_dataset = test_ds.map(preprocess_function, batched=True)\n",
        "tokenized_train_dataset = tokenized_train_dataset.remove_columns(['text', 'label'])\n",
        "tokenized_test_dataset = tokenized_test_dataset.remove_columns(['text', 'label'])\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n"
      ],
      "metadata": {
        "id": "l1hfi3KTC2hQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Lft4d9u5CC8r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3IxHw_NFJHI"
      },
      "outputs": [],
      "source": [
        "def generate_sample(text):\n",
        "  return f\"\"\"Perform Sentiment classification task.\n",
        "Given the text assign a sentiment label from ['negative', 'positive', 'neutral'].\n",
        "Return label only without any other text.\n",
        "\n",
        "<text>: {text}\n",
        "<sentiment>: \"\"\".strip()\n",
        "\n",
        "def preprocess_function(examples):\n",
        "  inputs = [generate_sample(text) for text in examples['text']]\n",
        "  model_inputs = tokenizer(inputs, max_length=512,  truncation=True)\n",
        "\n",
        "  # The labels are tokenized outputs\n",
        "  labels = tokenizer(text_target=examples['label'],\n",
        "                     max_length=512,\n",
        "                     truncation=True)\n",
        "\n",
        "  model_inputs['labels'] = labels['input_ids']\n",
        "\n",
        "  return model_inputs\n",
        "\n",
        "int2str = {-1: 'negative', 0: 'neutral', 1: 'positive'}\n",
        "str2int = {v:k for k, v in int2str.items()}\n",
        "\n",
        "train_df, test_df = pd.read_csv(\"train_data.csv\"), pd.read_csv(\"test_data.csv\")\n",
        "train_df['label'] = train_df['label'].map(int2str)\n",
        "test_df['label'] = test_df['label'].map(int2str)\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset =  Dataset.from_pandas(test_df)\n",
        "\n",
        "tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)\n",
        "tokenized_train_dataset = tokenized_train_dataset.remove_columns(['text', 'label'])\n",
        "tokenized_test_dataset = tokenized_test_dataset.remove_columns(['text', 'label'])\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "fkwoDr_OLang"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "6GxvXGz5KJFM",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "4065478bcfb048789b03f9c18f954bb4",
            "b2a5329eab7e4aba93121f2976e09d2f",
            "bdb42c36e19a4ea0ba351c0086cd402d",
            "53900bf72e4a4355bc8c0f059a150ba5",
            "8b77789ca5d44737a2b02b33959db4dd",
            "b9cce1679a7f41e5b99985cf15a51c34",
            "ebc20f5bf7bf49339ec8197aa1007bf1",
            "769d10133b554148885d04a4de3a94ca",
            "c161b9a219e940fba876d4c3b782a344",
            "0baad91600c14d459e43128baf3416d5",
            "5de78e7e1870452ebda57efc5781b82a"
          ]
        },
        "outputId": "811b1071-0271-41ab-9a7b-98fad35611fd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4065478bcfb048789b03f9c18f954bb4"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "nltk.download(\"punkt\", quiet=True)\n",
        "metric = evaluate.load(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "   preds, labels = eval_preds\n",
        "\n",
        "   # decode preds and labels\n",
        "   labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "   decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "   decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "   # rougeLSum expects newline after each sentence\n",
        "   decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "   decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "\n",
        "   result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "\n",
        "   return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "rKqJPbPHKnCb"
      },
      "outputs": [],
      "source": [
        "training_args = Seq2SeqTrainingArguments(\n",
        "   output_dir=\"./results\",\n",
        "   evaluation_strategy=\"epoch\",\n",
        "   learning_rate=2e-5,\n",
        "   per_device_train_batch_size=8,\n",
        "   per_device_eval_batch_size=8,\n",
        "   weight_decay=0.01,\n",
        "   save_total_limit=3,\n",
        "   num_train_epochs=3,\n",
        "   predict_with_generate=True,\n",
        "   push_to_hub=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Wgj-yxJbLN6s"
      },
      "outputs": [],
      "source": [
        "trainer = Seq2SeqTrainer(\n",
        "   model=model,\n",
        "   args=training_args,\n",
        "   train_dataset=tokenized_train_dataset,\n",
        "   eval_dataset=tokenized_test_dataset,\n",
        "   tokenizer=tokenizer,\n",
        "   data_collator=data_collator,\n",
        "   compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "-w4AHB7nMTFf",
        "outputId": "0db84d14-4aaf-47a8-ff3b-624612a6b69f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [21/21 01:38, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.338208</td>\n",
              "      <td>0.742857</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.742857</td>\n",
              "      <td>0.742857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.335532</td>\n",
              "      <td>0.742857</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.742857</td>\n",
              "      <td>0.742857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.334546</td>\n",
              "      <td>0.757143</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.757143</td>\n",
              "      <td>0.757143</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=21, training_loss=0.4268137613932292, metrics={'train_runtime': 104.8262, 'train_samples_per_second': 1.46, 'train_steps_per_second': 0.2, 'total_flos': 232120908103680.0, 'train_loss': 0.4268137613932292, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "80WlRuSLXNEm"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def run_on_test(test_dataset):\n",
        "  golden_labels = []\n",
        "  predicted_labels = []\n",
        "\n",
        "  test_texts, test_labels = test_dataset['text'], test_dataset['label']\n",
        "  inputs = [generate_sample(text) for text in test_texts]\n",
        "\n",
        "  # Running inference on 1 sample at a time to avoid OOM issue\n",
        "  for i, input in enumerate(tqdm(inputs)):\n",
        "    input = tokenizer(input, return_tensors='pt')\n",
        "    output = model.generate(**input)\n",
        "\n",
        "    golden_labels.append(str2int[test_labels[i]])\n",
        "    predicted_labels.append(str2int[tokenizer.decode(output[0], skip_special_tokens=True)])\n",
        "\n",
        "    #print(f'Generated label\" {tokenizer.decode(output[0], skip_special_tokens=True)}', end=\" | \")\n",
        "    #print(f'Golden label {test_labels[i]}', end='\\n\\n')\n",
        "  return golden_labels, predicted_labels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "golden_labels, predicted_labels = run_on_test(test_ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7aBT-AM8pSs",
        "outputId": "6d5dbe2d-2f49-4f6f-c79c-6681574628ec"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/70 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
            "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1273: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1591: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
            "  warnings.warn(\n",
            " 47%|████▋     | 33/70 [00:25<00:27,  1.36it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (687 > 512). Running this sequence through the model will result in indexing errors\n",
            "100%|██████████| 70/70 [00:47<00:00,  1.48it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(golden_labels, predicted_labels, digits=4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_gkSMN6EKE0",
        "outputId": "664b16f0-02f2-46e7-87b6-996ccdc5b40e"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "          -1     0.8095    0.8095    0.8095        21\n",
            "           0     0.8750    0.5385    0.6667        26\n",
            "           1     0.6667    0.9565    0.7857        23\n",
            "\n",
            "    accuracy                         0.7571        70\n",
            "   macro avg     0.7837    0.7682    0.7540        70\n",
            "weighted avg     0.7869    0.7571    0.7486        70\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---> (more parameters)\n",
        "\n",
        "macro\n",
        "\n",
        "0.7565 | 0.7870 | 0.8305\n",
        "\n",
        "micro\n",
        "\n",
        "0.7505 | 0.7825 | 0.8279"
      ],
      "metadata": {
        "id": "UDGK9rO7IJMa"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4065478bcfb048789b03f9c18f954bb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b2a5329eab7e4aba93121f2976e09d2f",
              "IPY_MODEL_bdb42c36e19a4ea0ba351c0086cd402d",
              "IPY_MODEL_53900bf72e4a4355bc8c0f059a150ba5"
            ],
            "layout": "IPY_MODEL_8b77789ca5d44737a2b02b33959db4dd"
          }
        },
        "b2a5329eab7e4aba93121f2976e09d2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9cce1679a7f41e5b99985cf15a51c34",
            "placeholder": "​",
            "style": "IPY_MODEL_ebc20f5bf7bf49339ec8197aa1007bf1",
            "value": "Downloading builder script: 100%"
          }
        },
        "bdb42c36e19a4ea0ba351c0086cd402d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_769d10133b554148885d04a4de3a94ca",
            "max": 6270,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c161b9a219e940fba876d4c3b782a344",
            "value": 6270
          }
        },
        "53900bf72e4a4355bc8c0f059a150ba5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0baad91600c14d459e43128baf3416d5",
            "placeholder": "​",
            "style": "IPY_MODEL_5de78e7e1870452ebda57efc5781b82a",
            "value": " 6.27k/6.27k [00:00&lt;00:00, 422kB/s]"
          }
        },
        "8b77789ca5d44737a2b02b33959db4dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b9cce1679a7f41e5b99985cf15a51c34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebc20f5bf7bf49339ec8197aa1007bf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "769d10133b554148885d04a4de3a94ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c161b9a219e940fba876d4c3b782a344": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0baad91600c14d459e43128baf3416d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5de78e7e1870452ebda57efc5781b82a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}