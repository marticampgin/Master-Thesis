{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(test_sample, num_few_shot_samples=3, few_shot_data=None):\n",
    "    \n",
    "    task_description = \"\"\"Perform Sentiment classification task.\n",
    "Given the text assign a sentiment label from ['positive', 'negative', 'neutral'].\n",
    "Return label only without any other text.\\n\"\"\"\n",
    "\n",
    "    for i in range(num_few_shot_samples + 1):\n",
    "        if i != num_few_shot_samples:\n",
    "            sample = few_shot_data[i]\n",
    "            text, label = sample['text'], sample['label']\n",
    "\n",
    "        if i == num_few_shot_samples:\n",
    "            task_description += f\"\\n<text>: {test_sample}\\n<sentiment>:\"\n",
    "        else:\n",
    "            task_description += f\"\\n<text>: {text}\\n<sentiment>: {label}\\n\"\n",
    "\n",
    "    return task_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_prompt_instances(train_dataset, num_samples, num_of_runs, seed):\n",
    "    \"\"\"\n",
    "    Function that samples few-shot instaces for each label\n",
    "    \"\"\"\n",
    "    positive_samples = []\n",
    "    neutral_samples = []\n",
    "    negative_samples = []\n",
    "\n",
    "    # Collect samples for each label\n",
    "    for sample in train_dataset:\n",
    "        if sample[\"label\"] == \"positive\":\n",
    "            positive_samples.append(sample)\n",
    "        elif sample[\"label\"] == \"neutral\":\n",
    "            neutral_samples.append(sample)\n",
    "        else:\n",
    "            negative_samples.append(sample)\n",
    "\n",
    "    random.seed(seed)\n",
    "    if num_samples in (1, 3):\n",
    "        pos_train_samples = random.choices(positive_samples, k=num_of_runs)\n",
    "        neut_train_samples = random.choices(neutral_samples, k=num_of_runs)\n",
    "        neg_train_samples = random.choices(negative_samples, k=num_of_runs)\n",
    "\n",
    "        if num_samples == 1:\n",
    "            # For num_of_runs of 1-shot testing sample each num_of_runs times \n",
    "            return pos_train_samples, neut_train_samples, neg_train_samples\n",
    "        \n",
    "        else:\n",
    "            exp1 = [pos_train_samples[0], neut_train_samples[0], neg_train_samples[0]]\n",
    "            exp2 = [pos_train_samples[1], neut_train_samples[1], neg_train_samples[1]]\n",
    "            exp3 = [pos_train_samples[2], neut_train_samples[2], neg_train_samples[2]]\n",
    "\n",
    "            return exp1, exp2, exp3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(test_data, chosen_samples, num_samples, client, str2int):\n",
    "\n",
    "    \"\"\"\n",
    "    Function that sends requests to ChatGPT model through API\n",
    "    \"\"\"\n",
    "    temperature = 0  # for determenistic outputs\n",
    "    predicted_labels = []\n",
    "    error_analysis_list = []  # for further error analysis \n",
    "\n",
    "    for text in tqdm(test_data):\n",
    "        prompt = generate_prompt(text, num_samples, chosen_samples)\n",
    "\n",
    "        completion  = client.chat.completions.create(\n",
    "            model='gpt-3.5-turbo-1106',\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            temperature=temperature\n",
    "        )\n",
    "        predicted_label = completion.choices[0].message.content\n",
    "        predicted_labels.append(str2int[predicted_label.strip()])\n",
    "        error_analysis_list.append({\"sample\": text, \"pred_label\": predicted_label})\n",
    "    \n",
    "    return predicted_labels, error_analysis_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_score(reports):\n",
    "    negative_prec, negative_rec, negative_f1 = [], [], []\n",
    "    positive_prec, positive_rec, positive_f1 = [], [], []\n",
    "    neutral_prec, neutral_rec, neutral_f1 = [], [], []\n",
    "    micro_f1, macro_f1 = [], []\n",
    "\n",
    "    for report in reports:\n",
    "        negative_prec.append(report['-1']['precision'])\n",
    "        negative_rec.append(report['-1']['recall'])\n",
    "        negative_f1.append(report['-1']['f1-score'])\n",
    "\n",
    "        positive_prec.append(report['1']['precision'])\n",
    "        positive_rec.append(report['1']['recall'])\n",
    "        positive_f1.append(report['1']['f1-score'])\n",
    "\n",
    "        neutral_prec.append(report['0']['precision'])\n",
    "        neutral_rec.append(report['0']['recall'])\n",
    "        neutral_f1.append(report['0']['f1-score'])\n",
    "\n",
    "        macro_f1.append(report['macro avg']['f1-score'])\n",
    "        micro_f1.append(report['weighted avg']['f1-score'])\n",
    "\n",
    "    avg_results = {'Negative precison': np.mean(negative_prec),\n",
    "                    'Negative recall': np.mean(negative_rec),\n",
    "                    'Negative f1-score': np.mean(negative_f1),\n",
    "                    'Netural precison': np.mean(neutral_prec),\n",
    "                    'Netural recall': np.mean(neutral_rec),\n",
    "                    'Netural f1-score': np.mean(neutral_f1),\n",
    "                    'Positive precison': np.mean(positive_prec),\n",
    "                    'Positive recall': np.mean(positive_rec),\n",
    "                    'Positive f1-score': np.mean(positive_f1),\n",
    "                    'Macro avg': np.mean(macro_f1), \n",
    "                    'Weighted avg': np.mean(micro_f1)}\n",
    "    \n",
    "    return avg_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(samples, num_samples, test_name, test_data, golden_labels, client, str2int):\n",
    "    reports = []\n",
    "\n",
    "    if num_samples == 1:\n",
    "        # One-shot learnign scenario \n",
    "        for sample in samples:\n",
    "            predicted_labels, err_list = run_test(test_data, [sample], num_samples=num_samples, client=client, str2int=str2int)\n",
    "            reports.append(classification_report(golden_labels, predicted_labels, output_dict=True))\n",
    "    else:\n",
    "        # Few-shot learning scenario \n",
    "        for sample in samples:\n",
    "            predicted_labels, err_list = run_test(test_data, sample, num_samples=num_samples, client=client, str2int=str2int)\n",
    "            reports.append(classification_report(golden_labels, predicted_labels, output_dict=True))\n",
    "        \n",
    "    avg_results = get_avg_score(reports)  # average the results \n",
    "\n",
    "    print(f\"Results for {test_name}, avg. for 3 runs:\")\n",
    "    for metric, result in avg_results.items():\n",
    "        print(f'{metric:25}{result:.4f}')\n",
    "\n",
    "    # Saving results for error-analysis\n",
    "    \n",
    "    with open(f\"{test_name}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for i, gold in enumerate(golden_labels):\n",
    "            f.write(f\"pred: {err_list[i]['pred_label']}, gold: {gold}\\n\")\n",
    "            f.write(f\" {err_list[i]['sample']}\\n\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"./data\"\n",
    "\n",
    "train_path = os.path.join(data_path, \"train_data.csv\")\n",
    "test_path = os.path.join(data_path, \"test_data.csv\")\n",
    "\n",
    "int2str = {-1: 'negative', 0: 'neutral', 1: 'positive'}\n",
    "str2int = {v:k for k, v in int2str.items()}\n",
    "\n",
    "train_df, test_df = pd.read_csv(train_path), pd.read_csv(test_path)\n",
    "train_df['label'] = train_df['label'].map(int2str)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df) \n",
    "test_dataset =  Dataset.from_pandas(test_df)\n",
    "\n",
    "test_texts, golden_labels = test_dataset['text'], test_dataset['label']\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = \"enter-key-here\"\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples formatted for one-shot experiments\n",
    "one_shot_all_samples = sample_prompt_instances(train_dataset, num_samples=1, num_of_runs=3, seed=7)\n",
    "\n",
    "# 1-shot positive-sample tests\n",
    "test(one_shot_all_samples[0], 1, \"1-shot exp. with positive sample\", test_texts, golden_labels, client, str2int)\n",
    "\n",
    "# 1-shot neut-sample tests\n",
    "test(one_shot_all_samples[1], 1, \"1-shot exp. with neutral sample\", test_texts, golden_labels, client, str2int)\n",
    "\n",
    "# 1-shot negative-sample tests\n",
    "test(one_shot_all_samples[2], 1, \"1-shot exp. with negative sample\", test_texts, golden_labels, client, str2int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples formatted for one-shot experiments\n",
    "three_shot_samples_test = sample_prompt_instances(train_dataset, num_samples=3, num_of_runs=3, seed=7)\n",
    "\n",
    "test(three_shot_samples_test, num_samples=3, test_name=\"3-shot exp.\", test_data=test_texts, golden_labels=golden_labels, client=client, str2int=str2int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
